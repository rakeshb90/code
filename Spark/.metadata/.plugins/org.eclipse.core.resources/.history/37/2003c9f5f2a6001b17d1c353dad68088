package com.project.loanApprovalPrediction;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.ml.feature.OneHotEncoder;
import org.apache.spark.ml.feature.StringIndexer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class LoanAmountPrediction {
	public static Dataset init() {
		System.setProperty("hadoop.home.dir", "C:/Users/Rakesh/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
		SparkSession spark=SparkSession.builder().appName("Loan Amount prediction")
				.master("local[*]").config("spark.some.config.option", "some-value")
				.getOrCreate();
		Dataset<Row> input=spark.read()
				.option("header", true)
				.option("inferSchema", true)
				.csv("src/main/resources/input/loan_prediction.csv");
		return input;
	}
	public static Dataset stringToInteger(Dataset csvData) {


		csvData=csvData.withColumnRenamed("LoanAmount", "label");
		StringIndexer GenderIndex=new StringIndexer()
				.setInputCol("Gender")
				.setOutputCol("GenderIndex")
				.setHandleInvalid("skip");
		csvData=GenderIndex.fit(csvData).transform(csvData);
		
		StringIndexer Self_EmployedIndex=new StringIndexer()
				.setInputCol("Self_Employed")
				.setOutputCol("Self_EmployedIndex")
				.setHandleInvalid("skip");
		csvData=Self_EmployedIndex.fit(csvData).transform(csvData);
		
		StringIndexer MarriedIndex=new StringIndexer()
				.setInputCol("Married")
				.setOutputCol("MarriedIndex")
				.setHandleInvalid("skip");
		csvData=MarriedIndex.fit(csvData).transform(csvData);
		
		StringIndexer EducationIndex=new StringIndexer()
				.setInputCol("Education")
				.setOutputCol("EducationIndex")
				.setHandleInvalid("skip");
		csvData=EducationIndex.fit(csvData).transform(csvData);
		
		StringIndexer Property_AreaIndex=new StringIndexer()
				.setInputCol("Property_Area")
				.setOutputCol("Property_AreaIndex")
				.setHandleInvalid("skip");
		csvData=Property_AreaIndex.fit(csvData).transform(csvData);
		
		StringIndexer DependentsIndex=new StringIndexer()
				.setInputCol("Dependents")
				.setOutputCol("DependentsIndex")
				.setHandleInvalid("skip");
		csvData=DependentsIndex.fit(csvData).transform(csvData);
     return csvData;
	}
	public static Dataset integerToVector(Dataset csvData) {
		
		OneHotEncoder encoder=new OneHotEncoder()
				.setInputCols(new String[] {"GenderIndex","Self_EmployedIndex","MarriedIndex","EducationIndex","Property_AreaIndex","DependentsIndex"})
				.setOutputCols(new String[] {"GenderVector","Self_EmployedVector","MarriedVector","EducationVector","Property_AreaVector","DependentsVector"});
		csvData=encoder.fit(csvData).transform(csvData);
		return csvData;

	}
	public static void trainedModel() {
		
	}
	public static void loanAmountPredict() {
		Dataset<Row> inputData=init();
		inputData=stringToInteger(inputData);
		inputData=integerToVector(inputData);
		VectorAssembler assembler=new VectorAssembler()
				.setInputCols(new String[] {"GenderVector","MarriedVector","EducationVector","Self_EmployedVector","ApplicantIncome","CoapplicantIncome","Loan_Amount_Term","Credit_History","Property_AreaVector"})
				.setOutputCol("features");
		Dataset<Row> modelInput=assembler.transform(inputData)
				.select("LoanAmount","features")
				.withColumnRenamed("LoanAmount", "label");
			
		Dataset<Row>[] splitData=modelInput.randomSplit(new double[] {0.8,0.2});
		Dataset<Row> traningAndTestData=splitData[0];
		Dataset<Row>  holdOutData=splitData[1];
		trainedModel(traningAndTestData);
		testModel(holdOutData);
		
	}
	
	

	public static void main(String[] args) {
	
	
		loanAmountPredict();

	}

}
