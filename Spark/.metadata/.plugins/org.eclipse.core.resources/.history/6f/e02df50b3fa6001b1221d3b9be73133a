package com.project.housepricepridiction;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.regression.LinearRegression;
import org.apache.spark.ml.regression.LinearRegressionModel;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
//import org.apache.spark.ml.feature.VectorAssembler;

public class Driver {
     
	public static void main(String[] args) {
        System.setProperty("hadoop.home.dir", "C:/Users/Rakesh/hadoop");
		Logger.getLogger("org.apache").setLevel(Level.WARN);
		SparkSession spark=SparkSession.builder().appName("House price prediction")
				.master("local[*]").config("spark.some.config.option", "some-value")
				.getOrCreate();
		Dataset<Row> input=spark.read()
				.option("header", true)
				.option("inferSchema", true)
				.csv("src/main/resources/input/kc_house_data.csv");
		System.out.println("**************House Price prediction project**************");
//		input.printSchema();
//		input.show();
		VectorAssembler vectorAssembler=new VectorAssembler()
				.setInputCols(new String[] {"bathrooms","bedrooms","sqft_living","sqft_lot","floors","waterfront","sqft_above","sqft_basement"})
				.setOutputCol("features");
		Dataset<Row> modelInput=vectorAssembler.transform(input)
				.select("price","features")
				.withColumnRenamed("price","label");
	   
//		modelInput.show();
		
		Dataset<Row>[] trainingAndTest=modelInput.randomSplit(new double[] {0.8,0.2});
		Dataset<Row> trainingData=trainingAndTest[0];
		Dataset<Row> testData=trainingAndTest[1];
		LinearRegressionModel model=new LinearRegression()
				.setMaxIter(10)
				.setRegParam(0.3)
				.setElasticNetParam(0.8)
				.fit(trainingData);
		System.out.println("R2: "+model.summary().r2()+" and Coff: "+model.summary().coefficientStandardErrors());
		model.transform(testData).show();
		
	}

}
